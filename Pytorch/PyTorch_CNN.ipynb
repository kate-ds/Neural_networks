{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Домашнее задание\n",
        "\n",
        "1. Обучите CNN (самописная) на CIFAR-100.\n",
        "2. Обучите CNN на CIFAR-100 через дообучение ImageNet Resnet-50.\n",
        "3. *Обучите CNN на CIFAR-100 через дообучение ImageNet Resnet-50 с аугментацией данных."
      ],
      "metadata": {
        "id": "UqEEXWo9_ma2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Самописная CNN"
      ],
      "metadata": {
        "id": "FFtnuueQYjY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn_s-pW4_cyW",
        "outputId": "c7ee835b-f16d-4ba9-c9c5-161ec3f8d71d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ffa43063c90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.CIFAR100(root='data/', train=True, download=True)\n",
        "\n",
        "\n",
        "class MyOwnCifar(torch.utils.data.Dataset):\n",
        "   \n",
        "    def __init__(self, init_dataset, transform=None):\n",
        "        self._base_dataset = init_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self._base_dataset[idx][0]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        lbl = self._base_dataset[idx][1]\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "\n",
        "def train_valid_split(Xt):\n",
        "    X_train, X_test = train_test_split(Xt, test_size=0.05, random_state=13)\n",
        "    return X_train, X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpvPhsv4_4_7",
        "outputId": "9cbe5ed3-ef78-4a55-d319-d91232cd1d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = dataset.classes"
      ],
      "metadata": {
        "id": "yrefAcHLEAsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_actions = transforms.Compose([transforms.Resize(44),\n",
        "                                    transforms.RandomCrop(32, padding=4), \n",
        "                                    transforms.ToTensor()])\n",
        "\n",
        "train_dataset, valid_dataset = train_valid_split(dataset)\n",
        "\n",
        "train_dataset = MyOwnCifar(train_dataset, trans_actions)\n",
        "valid_dataset = MyOwnCifar(valid_dataset, transforms.ToTensor())"
      ],
      "metadata": {
        "id": "War0U8ciAd5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=False,\n",
        "                          num_workers=1)"
      ],
      "metadata": {
        "id": "EmiZl8geAgRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img, lbl in train_loader:\n",
        "    print(img.size())\n",
        "    print(classes[lbl[0]])\n",
        "    plt.imshow(img[0].permute(1, 2, 0))\n",
        "    break"
      ],
      "metadata": {
        "id": "-BO4-DmwAmgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "f93cc4a8-65da-41a6-fa83-a639ad6f38eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32])\n",
            "mouse\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaL0lEQVR4nO2dW6xcZ3XH/2v2njl340uCZZJAgEaqAioBWRYVCHERKEVIIVIVkQeUhwijikig0ocolUoq9QGqAuKJyjQRoaKElIuIqqiQRkgRLwGHBicQmoTUwTa2T2wf2+c2tz2rDzOWjtP9X+d4rj58/59kec5e59t77W/vNXvO95+1lrk7hBB//FQm7YAQYjwo2IVIBAW7EImgYBciERTsQiSCgl2IRMgHGWxmtwL4GoAMwL+4+xc3+f2h6nxve9vbqM29Q22dTmAruA19yJRmVzxkMIZ+PH7O4WwQYyT1RvuL5zEwElPf09Sn/x4dkNmyYBB5TC+eWMSFcxdLB1q/OruZZQBeAPBhAMcB/ALAne7+m2DMUIP9+d/+mtoajQa11VfXqG19ZZUfsN0u3WzgbxDRKVcy/sGq76li44L7JrxJA2sneENtkzdNtr27P34sq/ATqGRZMK58jsnm7pjgvLxTUFsnuA/aGT+3okqcWeDnVZkrf05/9va/xovPvlQ6WYN8jD8A4CV3f9ndmwAeBnDbAPsTQoyQQYL9OgDHNvx8vLdNCHEVMtDf7FvBzA4CODjq4wghYgYJ9hMAbtjw8/W9bZfh7ocAHAKG/ze7EGLrDPIx/hcAbjKzN5tZDcAnADw6HLeEEMOm7ye7u7fN7B4AP0ZXenvQ3fny+JixaGU3WGGuBqu+lpfbzPl7ZqXS3wpzJDWFKg7ZZzQmkoU6kfQWfE5rkRMoghNrF8GKdbBCbgVfIWfXOg+ec+Qyd/cXLON7MMvRPDaJQtFplqs/AODV8v1F9/1Af7O7+2MAHhtkH0KI8aBv0AmRCAp2IRJBwS5EIijYhUgEBbsQiTDyb9CNEgukjkhqipKJECanMOmt32SiQJYLJKrARZrgEcmNkSwU5ADCA80uI/57kLTS6XA/2gWXoZqtFrUVrXJZzgMJzXJuy6JxoV565ZJYo8HPq+HEFsiXerILkQgKdiESQcEuRCIo2IVIBAW7EImwrVfjw3pmUeJEtDIaLtUTW78VpKIkmaj8WLgaXz4wOq1gERxFWIOO27JoGZ8RJAYVwUlnxm3NdvlqfLsIVrrbXDHIAikkWqnPgnPLyDO32qnSMe0m8T+4lnqyC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhG2tfTWT0cdIH6Hi+rC0RpjQUJIJE9FiRNMQuvaqImqV8HuNiE6t2gYqZEWjYpko7BeX9A5hcxVk5etQ7sTSIBB67BYCub7zLPyMKyAnxcT5aLkMD3ZhUgEBbsQiaBgFyIRFOxCJIKCXYhEULALkQgDSW9mdhTAMoACQNvd9w/DqWHQrywXJFAF++tDrkNcuy6SyiqRZMckrz7bSUXTEUpvJKMvnPtgh6EtuGiVrHxgJZDemm1+sFZgKzp8p4EJBZHlskhSJNl30b04DJ39A+5+Zgj7EUKMEH2MFyIRBg12B/ATM3vazA4OwyEhxGgY9GP8e939hJm9HsDjZvZbd39y4y/03gT0RiDEhBnoye7uJ3r/LwL4IYADJb9zyN33X02Ld0KkSN/BbmZzZrZw6TWAjwB4bliOCSGGyyAf4/cC+GEvcysH8G/u/p9D8WqrBBlIFlRRDN/hwuwqJncEhHIS95+1TwJi6Y2aApmv3zZakazITP1Kiu78qrWLQNcicxwdqxrYWoH/zRa/nuukDRUA1IktC7Le8mp56EZZln0Hu7u/DOAd/Y4XQowXSW9CJIKCXYhEULALkQgKdiESQcEuRCJs64KTHkhvodQUKkZXnsEWS1eRjb/Xhj72IV+F2Xd8d/1DMrk8yigr2tTWJj3bACDLA5mSqlfB/RE0xquEWWVRrzd+r662mqXbm811OqYgMrAH6XV6sguRCAp2IRJBwS5EIijYhUgEBbsQibCtV+PDwl4IVuoDogQJllsTtpPqO6GFrxaT8mO9ceXbI+Ei8iPs1hQkoBQkuaMIVuPbLb4a32o3qC2brVFbTiYr6MYUzn3UvioLEpsKsuIOAJ3VC6XbV1aW6Zj1Tvl8tJt8nvRkFyIRFOxCJIKCXYhEULALkQgKdiESQcEuRCJsa+nNA6kjTIQJk2SuXHbpt31SFhRkC2W5UHojPgZaU1j7LUhOaTW4zNNqlEtvnSKQNgN9sBP4kaFKbTlLDArmsAMuKboHyToNnriyvnSO2i6cebV0++LiIh1z9mK5XNdYr9MxerILkQgKdiESQcEuRCIo2IVIBAW7EImgYBciETaV3szsQQAfA7Do7m/vbdsN4LsAbgRwFMAd7r40OjfL6RRcqonq01mQERclgLHSZHFLo8DWp/RWCeRBhgd11azN56NV5/La8jl+yfOsPBMtr83QMdVpnr3GLUBe422SMmLqBHNYCbL5VpbLJS8AOH38BLUdffl/qe3s2fJ5XLrAs96WL66Ubo+u11ae7N8EcOtrtt0L4Al3vwnAE72fhRBXMZsGe6/f+mu/EXAbgId6rx8C8PEh+yWEGDL9/s2+191P9l6fQrejqxDiKmbgr8u6u1vwHVMzOwjg4KDHEUIMRr9P9tNmtg8Aev/TL/G6+yF33+/u+/s8lhBiCPQb7I8CuKv3+i4APxqOO0KIUbEV6e07AN4P4BozOw7gCwC+COARM7sbwCsA7hilk9S3oPifB5ltUUZcNIzJYcNu1QTE5xa3tiq3VSr8fb3eWKO2teWL1NZYXaW22kL5rZVXorZLQRYjV9dQya78Pmg1eXbY6tJZavvD739PbaeO/YHaloi8BgDrq+W+FOstOsabZK6C22bTYHf3O4npQ5uNFUJcPegbdEIkgoJdiERQsAuRCAp2IRJBwS5EImzrgpORPGUW9XqLbJGORo4XZFBFRTHbbV68MDq3sLccGxf0xbt4jktNF07zoof15fLMKwCokYy+LA+y+XJeODJQDtFs8HlsNsplreULXAo7d/I4tZ06xjPbls7xjLjgUsOMyJT5FB1TJVNlQTVSPdmFSAQFuxCJoGAXIhEU7EIkgoJdiERQsAuRCNtaeqtEWW+BvObOZahOEaQNETnPg95gRZtnLkXFAbMgI471LwN4Mc3GGpfJFo8dpbYzx49RW32ZZ71l3izd3sE+OmbadlCb5fxWXV7h53b+zJnS7WdOcQntYtBjrVEvPy8AAJHQAKA2M83H1cg9V+V6nVfKJcVKJulNiORRsAuRCAp2IRJBwS5EIijYhUiEbb0aj4KvdK+d58kdq+fLV2gBoL3GV5jRYcfjq/GtYDV+NUgkyVnfIgDVYGU6z4itw1d2V87x+WjV16mtaPOV6Xq9vK7dyoXzdMzyKq+Fd+5CsOJ+PtgnsdVXeG296SmegFKpzlFbVuXj8hk+rkoW42tB9kw2VX5dKuz6Q092IZJBwS5EIijYhUgEBbsQiaBgFyIRFOxCJMJW2j89COBjABbd/e29bfcD+BSAV3u/dp+7PzYqJxnLi6eobekUT+C4sMjb9LRXuSQDIjVF9dGi+nT1JpeuskB6i5JCKpVyW9T+qWgGCTmBH7XZWWpDVivdvL7Oz3l5nc/9seP8Wnc6QdJTUS6LWoUnpmQzPCFnapZLaLXpGWrLAznPybVpB7Lt1Fy5RJxVB5Pevgng1pLtX3X3W3r/xh7oQogrY9Ngd/cnAZwbgy9CiBEyyN/s95jZETN70Mx2Dc0jIcRI6DfYvw7grQBuAXASwJfZL5rZQTM7bGaH+zyWEGII9BXs7n7a3QvvdkD4BoADwe8ecvf97r6/XyeFEIPTV7Cb2cbaQrcDeG447gghRsVWpLfvAHg/gGvM7DiALwB4v5ndAsABHAXw6RH6SDn3yivUtnL2JLWtneMyTjuQf7xZnmmUB7JWFshkHshybdI+CQCKoMVP4eVSWZaXS2EAUI3kmiofF2V5oVYubdXbXCa7uMyz3pbO89ZKc4EcNk2yzaq1IENtboHvb26e2qI6c5WMX092j0R1FKuz5cfKo4xIarl0QPc7SzY/sNk4IcTVhb5BJ0QiKNiFSAQFuxCJoGAXIhEU7EIkwrYuOHnxFZ7ZNpXzIpA7iWwBAGtW3lYHANog2WEdfiyWdQUArWBcEbS2agdZXi1SozDLudQ0HUlNeZXaqlN8HhteLjW1wCUoD2S+2R08Ey0LfKxMl/tYneESWkbGAEARyKz1Fs9S81Ygo1XL/Z8K7lMmAVrgn57sQiSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSIRtLb1VuZqBhWkurVSneZbU7CyfklXy1tgOCjZ2Ankt61N6W2/woo1N0o/OC943LMp6m57hRSU959IQKuVyUh5k8wUKIIwPQ6vJz61CZLlqjct1FvjYWOM955pBgUiiRAIApmfKC1XWpgIpkhT7rAQZkXqyC5EICnYhEkHBLkQiKNiFSAQFuxCJsK1X4y1IqshJGyQAmAtXOfk4I1kmqyvLdEy9EazUg6/G16aC2nXgLZkajXIfK8EKcxYsdXcKLnk01spr8gGANcvPLcu575Wg5tpcjV+zRjAfRlbj80CBaAXKRavB6+RF1xqVoI0WObfo/o5W3emYKx4hhNiWKNiFSAQFuxCJoGAXIhEU7EIkgoJdiETYSvunGwB8C8BedNs9HXL3r5nZbgDfBXAjui2g7nD3pdG5+v+JkkWKNpe1KkECzcwcr3W2NlMuNa01+A7X17iPS6tcutozVZ4cAQBZziWZWlYuDVnQfsjbXDJaX+Ftl1bq/LwrWbmcNB3InlNBckqFJNYAQG5c1sqy8lu8WuVjWk0urxVtnoTUKXgiTPRUnSLSWxbUk6P3N7/dtvRkbwP4vLvfDODdAD5jZjcDuBfAE+5+E4Anej8LIa5SNg12dz/p7r/svV4G8DyA6wDcBuCh3q89BODjo3JSCDE4V/Q3u5ndCOCdAJ4CsNfdL7VKPYXux3whxFXKlr8ua2bzAL4P4HPuftE2fMXS3d2svP+wmR0EcHBQR4UQg7GlJ7uZVdEN9G+7+w96m0+b2b6efR+AxbKx7n7I3fe7+/5hOCyE6I9Ng926j/AHADzv7l/ZYHoUwF2913cB+NHw3RNCDIutfIx/D4BPAnjWzJ7pbbsPwBcBPGJmdwN4BcAdo3GR08i49FMvuJy0VucyVCuQtdpWXnOtUeHF007XeTupwy+corab3sjPbd8OLhvNkqwyD2ShZpC11yy4bX2Fn9v8fPmcVMl2AMg7vDZgK2h5Vc/5M6tGsv3yWiTX8f1ZoG1FWWpTQUup6any1lyBioZ6o1y27Tifp02D3d1/BtCz+NBm44UQVwf6Bp0QiaBgFyIRFOxCJIKCXYhEULALkQjbuuDk9J5d1NZuceltKSgaWKyvUlulWp55VQmKIeaktQ8ALNe5HHbmIvdjIciIW5gjvvAaiiiCopIWSF55IA7NT5fLSbNBO6la0JarOsuvdTbP91mZLr/FCw9aRvHbA/Umn/u8yvc5MxtkMZJCm+06vwdY9p0HLcX0ZBciERTsQiSCgl2IRFCwC5EICnYhEkHBLkQibGvpbWrPTmprLfNCiesXuSzXCvqXzZC3xrzKpbedO7jkcv2+a6htxwLPkqoROQkAqtPlcpgVPCOrsc6LKIL0bAOASlAQcXa+vHDnjt3X0jEzO/ZQ245dr+fHCu4Dz8rnY2XlPB1jZ4NCj4EU2WzweawGxTSNyGjtIFORZjEG/unJLkQiKNiFSAQFuxCJoGAXIhEU7EIkwrZeje8ECRAdD1YyA1utwmuTVUjfqGqFJ0Ds3clX6j9w4O3BsagJuXPFIG+X14zrtPmlrjf5Cm6rE7Q7Coqk5bPlq/E7972Jjtn9hjdS2/w8X3Gf3c3r2rmVqwmz5+fpmHbBFQgParzV13gGTVQDsF4nLbuidlLOfNRqvBDJo2AXIhEU7EIkgoJdiERQsAuRCAp2IRJhU+nNzG4A8C10WzI7gEPu/jUzux/ApwC82vvV+9z9sVE5WsZqi0tenaAVTyVqxTPL5TxrlkskFbIdAGpUIgH27OVyEkjbIgBotXhSRX2tXHppNbkEuLrOfcybQUujeZ7kc+0NN5Zuf8NbbqJjFvbwrt/1IMmkHmiARlo5ZUEdvzxoJ5VX+LE8aDlWX7nIbWvlteY6JEEGAKxTfu97m8fEVnT2NoDPu/svzWwBwNNm9njP9lV3/6ct7EMIMWG20uvtJICTvdfLZvY8gOtG7ZgQYrhc0d/sZnYjgHcCeKq36R4zO2JmD5oZr/UrhJg4Ww52M5sH8H0An3P3iwC+DuCtAG5B98n/ZTLuoJkdNrPDQ/BXCNEnWwp2M6uiG+jfdvcfAIC7n3b3wrtfFv4GgANlY939kLvvd/f9w3JaCHHlbBrsZmYAHgDwvLt/ZcP2fRt+7XYAzw3fPSHEsNjKavx7AHwSwLNm9kxv230A7jSzW9CV444C+PRIPAwoCi6DtAM5xgNbmC3XLrdVW3zMHListSPoyZTl5e2TAMCMy2jtolxS6oBn803N8qwxq3Lbwk5eT27X68tltKlp7rt1opprXNbqBO2rCnJt1pcDKWydZxVakPVWDR6da606tRXN8uOx2nShH4F/W1mN/xlQKlqPVVMXQgyGvkEnRCIo2IVIBAW7EImgYBciERTsQiTCti446R2ekRXJco0ml7yWWzyDrWiWyyfTgUTSMe5HvrJCbdMzXEJBsE/2/u3O39erNZ4FODXLbUxeAwB2aS4snaFjKkvn+P6CCpydGpcV1xvlstbyubN0TJShViHZZkBUBBIo2lw69Fa5rRLJaGR+eUToyS5EMijYhUgEBbsQiaBgFyIRFOxCJIKCXYhE2NbSm1nwXhXYOs6lq3orKGxYL7cVxgWPLJCMGhe49Dbf4BlgM9WgHx2R5Ypgfx5k7XWcH6te59lhv3vxBbZDOibPeCHNqYXy3nEAsFIvL9gIAPX18jm2Ns9C27Mr6B0XZOY1Aj/WV/m1LkhGXLXKw7OalV+X4FbUk12IVFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJsL2lNyI/AEAlsGWRzbiN0QyynVaCzDxeUhKYyrkfNdK/DACc+OJBppwZ97/V5JLRmdO/p7ZXz5Znji0FcuN6nWeUVYMefDMzvIjlroXyApzXvo73emuu8GyzSF5bDbLlOiRjEgCqWfk9Elxmep0j9GQXIhEU7EIkgoJdiERQsAuRCAp2IRJh09V4M5sG8CS6i8c5gO+5+xfM7M0AHgawB8DTAD7p7jyLZAREq+p5zpMqqlW+epsFyRh5pfx4HrQf4mkTsY+VuTk+rhaMK8gRgwQJqwSJPJ1gpd75uBqRGio5n6tOMFutNq8NOBeU5MvZ86zFB600AsVgdZnaGkFiUBasnudEeYlW3JnFwc9rK0/2BoAPuvs70G3PfKuZvRvAlwB81d3/BMASgLu3sC8hxITYNNi9y6W3umrvnwP4IIDv9bY/BODjI/FQCDEUttqfPet1cF0E8DiA3wE47+6XvgVxHMB1o3FRCDEMthTs7l64+y0ArgdwAMCfbvUAZnbQzA6b2eE+fRRCDIErWo139/MAfgrgzwHsNLNLC3zXAzhBxhxy9/3uvn8gT4UQA7FpsJvZtWa2s/d6BsCHATyPbtD/Ze/X7gLwo1E5KYQYnK0kwuwD8JCZZei+OTzi7v9hZr8B8LCZ/QOA/wbwwAj9LCUPpDercXnNO7ylUa3B01PapE1P0eRyB8lxAADk04EfO3ZS2/QsT+LIinL1s5EHl7rJWxOxNkMAkAW18GZfN1+6fdc1e+iYtTWu3EblBqezYP6L8nNrN3jSyupFLqE11rgEiKBd02yQrAMiYQaKLpzJwHzI5sHu7kcAvLNk+8vo/v0uhNgG6Bt0QiSCgl2IRFCwC5EICnYhEkHBLkQimAetkIZ+MLNXAbzS+/EaAGfGdnCO/Lgc+XE5282PN7n7tWWGsQb7ZQc2O3w1fKtOfsiPVPzQx3ghEkHBLkQiTDLYD03w2BuRH5cjPy7nj8aPif3NLoQYL/oYL0QiTCTYzexWM/sfM3vJzO6dhA89P46a2bNm9sw4i2uY2YNmtmhmz23YttvMHjezF3v/75qQH/eb2YnenDxjZh8dgx83mNlPzew3ZvZrM/tsb/tY5yTwY6xzYmbTZvZzM/tVz4+/721/s5k91Yub75pZkEpXgruP9R+ADN2yVm8BUAPwKwA3j9uPni9HAVwzgeO+D8C7ADy3Yds/Ari39/peAF+akB/3A/ibMc/HPgDv6r1eAPACgJvHPSeBH2OdE3RrAc/3XlcBPAXg3QAeAfCJ3vZ/BvBXV7LfSTzZDwB4yd1f9m7p6YcB3DYBPyaGuz8J4NxrNt+GbuFOYEwFPIkfY8fdT7r7L3uvl9EtjnIdxjwngR9jxbsMvcjrJIL9OgDHNvw8yWKVDuAnZva0mR2ckA+X2OvuJ3uvTwHYO0Ff7jGzI72P+SP/c2IjZnYjuvUTnsIE5+Q1fgBjnpNRFHlNfYHuve7+LgB/AeAzZva+STsEdN/ZERcdGSVfB/BWdHsEnATw5XEd2MzmAXwfwOfc/bJSMuOckxI/xj4nPkCRV8Ykgv0EgBs2/EyLVY4adz/R+38RwA8x2co7p81sHwD0/l+chBPufrp3o3UAfANjmhMzq6IbYN929x/0No99Tsr8mNSc9I59xUVeGZMI9l8AuKm3slgD8AkAj47bCTObM7OFS68BfATAc/GokfIouoU7gQkW8LwUXD1uxxjmxMwM3RqGz7v7VzaYxjonzI9xz8nIiryOa4XxNauNH0V3pfN3AP52Qj68BV0l4FcAfj1OPwB8B92Pgy10//a6G92eeU8AeBHAfwHYPSE//hXAswCOoBts+8bgx3vR/Yh+BMAzvX8fHfecBH6MdU4A/Bm6RVyPoPvG8ncb7tmfA3gJwL8DmLqS/eobdEIkQuoLdEIkg4JdiERQsAuRCAp2IRJBwS5EIijYhUgEBbsQiaBgFyIR/g/X1wcvVwiWLwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "czj9wR9rArtD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "661c350d-b4d9-4aec-9f47-b0a1687cdd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.dp_one = nn.Dropout(0.25)\n",
        "        self.dp_two = nn.Dropout(0.15)\n",
        "        \n",
        "        self.bn_one = torch.nn.BatchNorm2d(3) \n",
        "        self.conv_one = torch.nn.Conv2d(3, 100, 3)\n",
        "        self.bn_two = torch.nn.BatchNorm2d(100) \n",
        "        self.conv_two = torch.nn.Conv2d(100, 120, 3)\n",
        "        self.bn_three = torch.nn.BatchNorm2d(120)\n",
        "        self.conv_three = torch.nn.Conv2d(120, 150, 3)\n",
        "        self.bn_four = torch.nn.BatchNorm2d(150)\n",
        "        self.fc1 = torch.nn.Linear(600, 200)\n",
        "        self.fc2 = torch.nn.Linear(200, 120)\n",
        "        self.out = torch.nn.Linear(120, 100)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.bn_one(x)\n",
        "        x = self.conv_one(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        \n",
        "        x = self.bn_two(x)\n",
        "        x = self.conv_two(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        \n",
        "        x = self.bn_three(x)\n",
        "        x = self.conv_three(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        \n",
        "        x = self.bn_four(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dp_one(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dp_two(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        return self.out(x)\n",
        "       \n",
        "net = Net().to(device)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "uEwclGmmAwEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6558c189-bc3b-47e5-e19a-ab35bd315356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (dp_one): Dropout(p=0.25, inplace=False)\n",
            "  (dp_two): Dropout(p=0.15, inplace=False)\n",
            "  (bn_one): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_one): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn_two): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_two): Conv2d(100, 120, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn_three): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_three): Conv2d(120, 150, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn_four): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=600, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=120, bias=True)\n",
            "  (out): Linear(in_features=120, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "gLSleqN_AzVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "net.train()\n",
        "\n",
        "for epoch in range(num_epochs):  \n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        running_loss += loss.item()\n",
        "        running_items += len(labels)\n",
        "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
        "        \n",
        "        # выводим статистику о процессе обучения\n",
        "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
        "            net.eval()\n",
        "            \n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
        "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "                  f'Loss: {running_loss / running_items:.3f}. ' \\\n",
        "                  f'Acc: {running_right / running_items:.3f}', end='. ')\n",
        "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "\n",
        "            test_running_right, test_running_total = 0.0, 0.0\n",
        "            for i, data in enumerate(valid_loader):\n",
        "            \n",
        "                test_outputs = net(data[0].to(device))\n",
        "                test_running_total += len(data[1])\n",
        "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum()\n",
        "            \n",
        "            print(f'Test acc: {test_running_right / test_running_total:.3f}')\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "id": "rtCjUx1LA2Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebed314c-990f-41c2-d706-b3306616724a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25]. Step [1/372]. Loss: 0.036. Acc: 0.008. Test acc: 0.008\n",
            "Epoch [1/25]. Step [301/372]. Loss: 0.033. Acc: 0.046. Test acc: 0.062\n",
            "Epoch [2/25]. Step [1/372]. Loss: 0.033. Acc: 0.039. Test acc: 0.059\n",
            "Epoch [2/25]. Step [301/372]. Loss: 0.031. Acc: 0.076. Test acc: 0.100\n",
            "Epoch [3/25]. Step [1/372]. Loss: 0.031. Acc: 0.102. Test acc: 0.079\n",
            "Epoch [3/25]. Step [301/372]. Loss: 0.030. Acc: 0.091. Test acc: 0.097\n",
            "Epoch [4/25]. Step [1/372]. Loss: 0.030. Acc: 0.094. Test acc: 0.100\n",
            "Epoch [4/25]. Step [301/372]. Loss: 0.030. Acc: 0.105. Test acc: 0.129\n",
            "Epoch [5/25]. Step [1/372]. Loss: 0.030. Acc: 0.117. Test acc: 0.124\n",
            "Epoch [5/25]. Step [301/372]. Loss: 0.029. Acc: 0.116. Test acc: 0.145\n",
            "Epoch [6/25]. Step [1/372]. Loss: 0.029. Acc: 0.117. Test acc: 0.104\n",
            "Epoch [6/25]. Step [301/372]. Loss: 0.029. Acc: 0.127. Test acc: 0.133\n",
            "Epoch [7/25]. Step [1/372]. Loss: 0.029. Acc: 0.172. Test acc: 0.124\n",
            "Epoch [7/25]. Step [301/372]. Loss: 0.028. Acc: 0.132. Test acc: 0.150\n",
            "Epoch [8/25]. Step [1/372]. Loss: 0.028. Acc: 0.156. Test acc: 0.141\n",
            "Epoch [8/25]. Step [301/372]. Loss: 0.028. Acc: 0.141. Test acc: 0.137\n",
            "Epoch [9/25]. Step [1/372]. Loss: 0.029. Acc: 0.094. Test acc: 0.138\n",
            "Epoch [9/25]. Step [301/372]. Loss: 0.028. Acc: 0.148. Test acc: 0.158\n",
            "Epoch [10/25]. Step [1/372]. Loss: 0.027. Acc: 0.148. Test acc: 0.168\n",
            "Epoch [10/25]. Step [301/372]. Loss: 0.028. Acc: 0.157. Test acc: 0.151\n",
            "Epoch [11/25]. Step [1/372]. Loss: 0.029. Acc: 0.125. Test acc: 0.140\n",
            "Epoch [11/25]. Step [301/372]. Loss: 0.027. Acc: 0.164. Test acc: 0.172\n",
            "Epoch [12/25]. Step [1/372]. Loss: 0.027. Acc: 0.156. Test acc: 0.160\n",
            "Epoch [12/25]. Step [301/372]. Loss: 0.027. Acc: 0.163. Test acc: 0.171\n",
            "Epoch [13/25]. Step [1/372]. Loss: 0.026. Acc: 0.203. Test acc: 0.162\n",
            "Epoch [13/25]. Step [301/372]. Loss: 0.027. Acc: 0.171. Test acc: 0.165\n",
            "Epoch [14/25]. Step [1/372]. Loss: 0.026. Acc: 0.148. Test acc: 0.165\n",
            "Epoch [14/25]. Step [301/372]. Loss: 0.027. Acc: 0.173. Test acc: 0.166\n",
            "Epoch [15/25]. Step [1/372]. Loss: 0.026. Acc: 0.117. Test acc: 0.166\n",
            "Epoch [15/25]. Step [301/372]. Loss: 0.026. Acc: 0.181. Test acc: 0.160\n",
            "Epoch [16/25]. Step [1/372]. Loss: 0.026. Acc: 0.164. Test acc: 0.153\n",
            "Epoch [16/25]. Step [301/372]. Loss: 0.026. Acc: 0.183. Test acc: 0.176\n",
            "Epoch [17/25]. Step [1/372]. Loss: 0.026. Acc: 0.172. Test acc: 0.146\n",
            "Epoch [17/25]. Step [301/372]. Loss: 0.026. Acc: 0.187. Test acc: 0.167\n",
            "Epoch [18/25]. Step [1/372]. Loss: 0.024. Acc: 0.250. Test acc: 0.178\n",
            "Epoch [18/25]. Step [301/372]. Loss: 0.026. Acc: 0.192. Test acc: 0.175\n",
            "Epoch [19/25]. Step [1/372]. Loss: 0.028. Acc: 0.195. Test acc: 0.145\n",
            "Epoch [19/25]. Step [301/372]. Loss: 0.026. Acc: 0.193. Test acc: 0.164\n",
            "Epoch [20/25]. Step [1/372]. Loss: 0.025. Acc: 0.203. Test acc: 0.168\n",
            "Epoch [20/25]. Step [301/372]. Loss: 0.026. Acc: 0.196. Test acc: 0.171\n",
            "Epoch [21/25]. Step [1/372]. Loss: 0.024. Acc: 0.234. Test acc: 0.191\n",
            "Epoch [21/25]. Step [301/372]. Loss: 0.026. Acc: 0.197. Test acc: 0.136\n",
            "Epoch [22/25]. Step [1/372]. Loss: 0.024. Acc: 0.281. Test acc: 0.171\n",
            "Epoch [22/25]. Step [301/372]. Loss: 0.026. Acc: 0.203. Test acc: 0.155\n",
            "Epoch [23/25]. Step [1/372]. Loss: 0.024. Acc: 0.289. Test acc: 0.168\n",
            "Epoch [23/25]. Step [301/372]. Loss: 0.025. Acc: 0.207. Test acc: 0.165\n",
            "Epoch [24/25]. Step [1/372]. Loss: 0.026. Acc: 0.188. Test acc: 0.182\n",
            "Epoch [24/25]. Step [301/372]. Loss: 0.025. Acc: 0.204. Test acc: 0.157\n",
            "Epoch [25/25]. Step [1/372]. Loss: 0.025. Acc: 0.211. Test acc: 0.184\n",
            "Epoch [25/25]. Step [301/372]. Loss: 0.025. Acc: 0.211. Test acc: 0.171\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Дообучение Resnet-50"
      ],
      "metadata": {
        "id": "isEWK6N8P29C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "print(resnet50)"
      ],
      "metadata": {
        "id": "TOEr09CfA53e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870d3560-1f2e-46f0-dddd-5e8d9dd62a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(resnet50.to(device), input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPfRKWNkNxFc",
        "outputId": "d32af096-e299-4049-ae10-64f748e8a25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
            "             ReLU-81          [-1, 256, 28, 28]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-89         [-1, 1024, 14, 14]               0\n",
            "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
            "             ReLU-93          [-1, 256, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "             ReLU-96          [-1, 256, 14, 14]               0\n",
            "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-99         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-109         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "            ReLU-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
            "            ReLU-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-143          [-1, 512, 14, 14]               0\n",
            "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-146            [-1, 512, 7, 7]               0\n",
            "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-151           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-155            [-1, 512, 7, 7]               0\n",
            "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-158            [-1, 512, 7, 7]               0\n",
            "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-161           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-165            [-1, 512, 7, 7]               0\n",
            "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-168            [-1, 512, 7, 7]               0\n",
            "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-171           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                 [-1, 1000]       2,049,000\n",
            "================================================================\n",
            "Total params: 25,557,032\n",
            "Trainable params: 25,557,032\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 286.56\n",
            "Params size (MB): 97.49\n",
            "Estimated Total Size (MB): 384.62\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in list(resnet50.parameters())[:]:\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "Rq48hdxpN1P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50.fc = nn.Linear(2048, 100)\n",
        "\n",
        "summary(resnet50.to(device), input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DQVelUTN4Nm",
        "outputId": "ad8022e7-9a3a-4b88-8a70-bea7f22104ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
            "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
            "             ReLU-15            [-1, 256, 8, 8]               0\n",
            "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
            "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
            "             ReLU-19             [-1, 64, 8, 8]               0\n",
            "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
            "             ReLU-22             [-1, 64, 8, 8]               0\n",
            "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
            "             ReLU-25            [-1, 256, 8, 8]               0\n",
            "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
            "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
            "             ReLU-29             [-1, 64, 8, 8]               0\n",
            "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
            "             ReLU-32             [-1, 64, 8, 8]               0\n",
            "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "             ReLU-35            [-1, 256, 8, 8]               0\n",
            "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
            "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "             ReLU-39            [-1, 128, 8, 8]               0\n",
            "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
            "             ReLU-42            [-1, 128, 4, 4]               0\n",
            "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-47            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-57            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
            "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
            "             ReLU-61            [-1, 128, 4, 4]               0\n",
            "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
            "             ReLU-64            [-1, 128, 4, 4]               0\n",
            "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-67            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
            "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
            "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
            "             ReLU-71            [-1, 128, 4, 4]               0\n",
            "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
            "             ReLU-74            [-1, 128, 4, 4]               0\n",
            "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-77            [-1, 512, 4, 4]               0\n",
            "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
            "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
            "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
            "             ReLU-81            [-1, 256, 4, 4]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
            "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
            "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-89           [-1, 1024, 2, 2]               0\n",
            "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
            "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
            "             ReLU-93            [-1, 256, 2, 2]               0\n",
            "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
            "             ReLU-96            [-1, 256, 2, 2]               0\n",
            "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-99           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
            "            ReLU-103            [-1, 256, 2, 2]               0\n",
            "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
            "            ReLU-106            [-1, 256, 2, 2]               0\n",
            "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-109           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
            "            ReLU-113            [-1, 256, 2, 2]               0\n",
            "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
            "            ReLU-116            [-1, 256, 2, 2]               0\n",
            "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-119           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
            "            ReLU-123            [-1, 256, 2, 2]               0\n",
            "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
            "            ReLU-126            [-1, 256, 2, 2]               0\n",
            "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-129           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
            "            ReLU-133            [-1, 256, 2, 2]               0\n",
            "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
            "            ReLU-136            [-1, 256, 2, 2]               0\n",
            "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-139           [-1, 1024, 2, 2]               0\n",
            "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
            "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-143            [-1, 512, 2, 2]               0\n",
            "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-146            [-1, 512, 1, 1]               0\n",
            "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
            "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-151           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-155            [-1, 512, 1, 1]               0\n",
            "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-158            [-1, 512, 1, 1]               0\n",
            "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-161           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
            "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-165            [-1, 512, 1, 1]               0\n",
            "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-168            [-1, 512, 1, 1]               0\n",
            "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
            "            ReLU-171           [-1, 2048, 1, 1]               0\n",
            "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                  [-1, 100]         204,900\n",
            "================================================================\n",
            "Total params: 23,712,932\n",
            "Trainable params: 204,900\n",
            "Non-trainable params: 23,508,032\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.86\n",
            "Params size (MB): 90.46\n",
            "Estimated Total Size (MB): 96.33\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = resnet50.to(device)"
      ],
      "metadata": {
        "id": "Mkg1zfc-N8eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_actions = transforms.Compose([transforms.Resize(256),\n",
        "                                    transforms.RandomCrop(224, padding=4), \n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                         std=[0.229, 0.224, 0.225])])\n",
        "valid_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                            std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = train_valid_split(dataset)\n",
        "\n",
        "train_dataset = MyOwnCifar(train_dataset, train_actions)\n",
        "valid_dataset = MyOwnCifar(valid_dataset, valid_transforms)"
      ],
      "metadata": {
        "id": "G6_XgShKOAzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=1)"
      ],
      "metadata": {
        "id": "yXXDE769OG_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = []\n",
        "for name, param in resnet50.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-Fe6lDniOOIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "resnet50.train()\n",
        "\n",
        "for epoch in range(num_epochs):  \n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        running_loss += loss.item()\n",
        "        running_items += len(labels)\n",
        "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
        "        \n",
        "        # выводим статистику о процессе обучения\n",
        "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
        "            resnet50.eval()\n",
        "            \n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
        "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "                  f'Loss: {running_loss / running_items:.3f}. ' \\\n",
        "                  f'Acc: {running_right / running_items:.3f}', end='. ')\n",
        "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "\n",
        "            test_running_right, test_running_total = 0.0, 0.0\n",
        "            for i, data in enumerate(valid_loader):\n",
        "            \n",
        "                test_outputs = resnet50(data[0].to(device))\n",
        "                test_running_total += len(data[1])\n",
        "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum()\n",
        "            \n",
        "            print(f'Test acc: {test_running_right / test_running_total:.3f}')\n",
        "\n",
        "        resnet50.train()\n",
        "        \n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOM0j_gcOTsw",
        "outputId": "565b56fc-e2a8-41d7-e44c-3c9f9d6fe729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25]. Step [1/372]. Loss: 0.036. Acc: 0.000. Test acc: 0.012\n",
            "Epoch [1/25]. Step [301/372]. Loss: 0.020. Acc: 0.392. Test acc: 0.521\n",
            "Epoch [2/25]. Step [1/372]. Loss: 0.015. Acc: 0.562. Test acc: 0.529\n",
            "Epoch [2/25]. Step [301/372]. Loss: 0.013. Acc: 0.539. Test acc: 0.554\n",
            "Epoch [3/25]. Step [1/372]. Loss: 0.014. Acc: 0.516. Test acc: 0.540\n",
            "Epoch [3/25]. Step [301/372]. Loss: 0.013. Acc: 0.560. Test acc: 0.561\n",
            "Epoch [4/25]. Step [1/372]. Loss: 0.011. Acc: 0.594. Test acc: 0.551\n",
            "Epoch [4/25]. Step [301/372]. Loss: 0.012. Acc: 0.578. Test acc: 0.572\n",
            "Epoch [5/25]. Step [1/372]. Loss: 0.013. Acc: 0.594. Test acc: 0.566\n",
            "Epoch [5/25]. Step [301/372]. Loss: 0.011. Acc: 0.592. Test acc: 0.577\n",
            "Epoch [6/25]. Step [1/372]. Loss: 0.010. Acc: 0.633. Test acc: 0.572\n",
            "Epoch [6/25]. Step [301/372]. Loss: 0.011. Acc: 0.598. Test acc: 0.585\n",
            "Epoch [7/25]. Step [1/372]. Loss: 0.010. Acc: 0.609. Test acc: 0.578\n",
            "Epoch [7/25]. Step [301/372]. Loss: 0.011. Acc: 0.604. Test acc: 0.576\n",
            "Epoch [8/25]. Step [1/372]. Loss: 0.011. Acc: 0.562. Test acc: 0.569\n",
            "Epoch [8/25]. Step [301/372]. Loss: 0.011. Acc: 0.609. Test acc: 0.582\n",
            "Epoch [9/25]. Step [1/372]. Loss: 0.011. Acc: 0.578. Test acc: 0.577\n",
            "Epoch [9/25]. Step [301/372]. Loss: 0.011. Acc: 0.610. Test acc: 0.601\n",
            "Epoch [10/25]. Step [1/372]. Loss: 0.010. Acc: 0.633. Test acc: 0.593\n",
            "Epoch [10/25]. Step [301/372]. Loss: 0.011. Acc: 0.615. Test acc: 0.594\n",
            "Epoch [11/25]. Step [1/372]. Loss: 0.010. Acc: 0.672. Test acc: 0.586\n",
            "Epoch [11/25]. Step [301/372]. Loss: 0.011. Acc: 0.615. Test acc: 0.597\n",
            "Epoch [12/25]. Step [1/372]. Loss: 0.012. Acc: 0.609. Test acc: 0.586\n",
            "Epoch [12/25]. Step [301/372]. Loss: 0.010. Acc: 0.624. Test acc: 0.591\n",
            "Epoch [13/25]. Step [1/372]. Loss: 0.010. Acc: 0.656. Test acc: 0.578\n",
            "Epoch [13/25]. Step [301/372]. Loss: 0.010. Acc: 0.624. Test acc: 0.602\n",
            "Epoch [14/25]. Step [1/372]. Loss: 0.010. Acc: 0.609. Test acc: 0.576\n",
            "Epoch [14/25]. Step [301/372]. Loss: 0.010. Acc: 0.634. Test acc: 0.599\n",
            "Epoch [15/25]. Step [1/372]. Loss: 0.010. Acc: 0.625. Test acc: 0.580\n",
            "Epoch [15/25]. Step [301/372]. Loss: 0.010. Acc: 0.633. Test acc: 0.590\n",
            "Epoch [16/25]. Step [1/372]. Loss: 0.010. Acc: 0.672. Test acc: 0.596\n",
            "Epoch [16/25]. Step [301/372]. Loss: 0.010. Acc: 0.635. Test acc: 0.604\n",
            "Epoch [17/25]. Step [1/372]. Loss: 0.008. Acc: 0.680. Test acc: 0.586\n",
            "Epoch [17/25]. Step [301/372]. Loss: 0.010. Acc: 0.641. Test acc: 0.600\n",
            "Epoch [18/25]. Step [1/372]. Loss: 0.010. Acc: 0.602. Test acc: 0.586\n",
            "Epoch [18/25]. Step [301/372]. Loss: 0.010. Acc: 0.642. Test acc: 0.605\n",
            "Epoch [19/25]. Step [1/372]. Loss: 0.008. Acc: 0.750. Test acc: 0.589\n",
            "Epoch [19/25]. Step [301/372]. Loss: 0.010. Acc: 0.643. Test acc: 0.608\n",
            "Epoch [20/25]. Step [1/372]. Loss: 0.009. Acc: 0.688. Test acc: 0.604\n",
            "Epoch [20/25]. Step [301/372]. Loss: 0.010. Acc: 0.643. Test acc: 0.590\n",
            "Epoch [21/25]. Step [1/372]. Loss: 0.009. Acc: 0.648. Test acc: 0.592\n",
            "Epoch [21/25]. Step [301/372]. Loss: 0.010. Acc: 0.647. Test acc: 0.602\n",
            "Epoch [22/25]. Step [1/372]. Loss: 0.008. Acc: 0.656. Test acc: 0.602\n",
            "Epoch [22/25]. Step [301/372]. Loss: 0.010. Acc: 0.650. Test acc: 0.607\n",
            "Epoch [23/25]. Step [1/372]. Loss: 0.009. Acc: 0.656. Test acc: 0.585\n",
            "Epoch [23/25]. Step [301/372]. Loss: 0.009. Acc: 0.650. Test acc: 0.602\n",
            "Epoch [24/25]. Step [1/372]. Loss: 0.008. Acc: 0.680. Test acc: 0.602\n",
            "Epoch [24/25]. Step [301/372]. Loss: 0.010. Acc: 0.652. Test acc: 0.602\n",
            "Epoch [25/25]. Step [1/372]. Loss: 0.008. Acc: 0.648. Test acc: 0.577\n",
            "Epoch [25/25]. Step [301/372]. Loss: 0.009. Acc: 0.654. Test acc: 0.606\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобученная сеть показывает лучшую точность, чем самописная"
      ],
      "metadata": {
        "id": "KKqesWorrWfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PY9hLyaQbIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}